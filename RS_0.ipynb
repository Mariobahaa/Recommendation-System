{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RS.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mariobahaa/Recommendation-System/blob/master/RS_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYRf9c4m0jTg",
        "colab_type": "code",
        "outputId": "adef9991-c88f-4a45-eaf3-9a52283f4269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbqOMAOXwsgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# convert from json to pandas\n",
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    yield json.loads(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "# make dict and fit it by ids\n",
        "def dict_ids(col):\n",
        "  id = 0\n",
        "  dic = dict() \n",
        "  for i in col:\n",
        "    if i in dic:\n",
        "      dic[i] = dic.get(i)\n",
        "    else:\n",
        "      dic[i] = id\n",
        "      id += 1\n",
        "  return dic\n",
        "\n",
        "# make new col by mapping last ids to new ids\n",
        "def new_col(col,dic):\n",
        "  lis = list()\n",
        "  for i in col:\n",
        "    #if i in dic:\n",
        "    lis.append(dic.get(i))\n",
        "  return lis\n",
        "\n",
        "#create products per user map\n",
        "def productsOfUser(userId, productId):\n",
        "  productsOfUserMap = dict()\n",
        "\n",
        "  for i in range(0,len(userId)):\n",
        "    productsOfUserMap[userId[i]] = np.array([]).astype(int)\n",
        "\n",
        "  for i in range(0,len(userId)): \n",
        "    if productId[i] > -1:\n",
        "      productsOfUserMap[userId[i]] = np.append(productsOfUserMap[userId[i]] ,productId[i])\n",
        "        #productsOfUserMap[userId[i]].append([productId[i],cateId[i]])\n",
        "\n",
        "  return productsOfUserMap\n",
        "\n",
        "# def productsOfUser(userId, productId):\n",
        "#   productsOfUserMap = dict()\n",
        "\n",
        "#   for i in range(0,len(userId)): \n",
        "#     if productId[i] > -1:\n",
        "#       productsOfUserMap[userId[i]].append(productId[i])\n",
        "#         #productsOfUserMap[userId[i]].append([productId[i],cateId[i]])\n",
        "\n",
        "#   return productsOfUserMap\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvD8BS5BxVgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_all_beauty = getDF('/content/drive/My Drive/All Beauty/All_Beauty_5.json.gz')\n",
        "\n",
        "userId = np.array(df_all_beauty['reviewerID'])\n",
        "dict_all_beauty_users = dict_ids(userId)\n",
        "\n",
        "\n",
        "productId = np.array(df_all_beauty['asin'])\n",
        "dict_all_beauty_products = dict_ids(productId)\n",
        "\n",
        "userId =new_col(userId,dict_all_beauty_users)\n",
        "productId =new_col(productId,dict_all_beauty_products)\n",
        "\n",
        "categoryId = np.ones(len(productId)).astype(int)\n",
        "\n",
        "prodOfUser = productsOfUser(userId, productId)\n",
        "cateOfUser = productsOfUser(userId, categoryId)\n",
        "\n",
        "numberOfProducts = 0\n",
        "numberOfCategories = 0\n",
        "\n",
        "for i in prodOfUser:\n",
        "  numberOfProducts += 1\n",
        "\n",
        "for i in cateOfUser:\n",
        "  numberOfCategories += 1\n",
        "\n",
        "#print(prodOfUser)\n",
        "# print(cateOfUser)\n",
        "# print(categoryId)\n",
        "#print(userId)\n",
        "# print(df_all_beauty['userID'])\n",
        "#print(df_all_beauty['reviewerID'])\n",
        "# print(df_all_beauty['reviewerID'])\n",
        "# print(productId)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB4v_Z0Snk8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prodId = new_col(userId, prodOfUser)\n",
        "# cateId = new_col(userId, cateOfUser)\n",
        "\n",
        "\n",
        "userId = np.empty([0,1]).astype(int)\n",
        "\n",
        "\n",
        "for i in prodOfUser:\n",
        "  userId = np.append(userId, i)\n",
        "\n",
        "#len(userId),1\n",
        "prodId =[None]*len(userId)\n",
        "cateId =[None]*len(userId)\n",
        "\n",
        "# for i in range(0,len(userId)):\n",
        "#   prodId[i] = []\n",
        "#   cateId[i] = []\n",
        "# prodId = userId\n",
        "# cateId = userId\n",
        "\n",
        "#print(prodId )\n",
        "\n",
        "for i in range(0,len(userId)):\n",
        "  prod  = list(prodOfUser[i].reshape(len(prodOfUser[i])))\n",
        "  #print(prod)\n",
        "  prodId[i]= prod\n",
        "  cate = list(cateOfUser[i].reshape(len(cateOfUser[i])))\n",
        "  cateId[i] = cate\n",
        "\n",
        "#print(prodId)\n",
        "#print(cateId)\n",
        "candProd = np.array([]).astype(int)\n",
        "candCate = np.array([]).astype(int)\n",
        "\n",
        "for i in range(0,len(userId)):\n",
        "  candProd = prodId[i-1]\n",
        "\n",
        "#print(userId)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CTthdq19D02",
        "colab_type": "code",
        "outputId": "bd179bd8-c09d-4311-fb16-c7cbb09879db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "LastItemCatList =[]\n",
        "LastItemProdList =[]\n",
        "\n",
        "print(prodId[0])\n",
        "\n",
        "for sList in cateId:\n",
        "  LastItemCatList.append(sList[-1])\n",
        "  sList.pop()\n",
        "\n",
        "for sList in prodId:\n",
        "  LastItemProdList.append(sList[-1])\n",
        "  sList.pop()\n",
        "  # print(LastItemCatList)\n",
        "\n",
        "candProd=(np.array(LastItemProdList))\n",
        "candCate=(np.array(LastItemCatList))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 7, 29, 29, 63]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCYBneDKPN1O",
        "colab_type": "code",
        "outputId": "4c983aac-e54a-4af8-920b-979dc0c82f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "prodId = np.array(prodId)\n",
        "\n",
        "for i in range(len(prodId)):\n",
        "  prodId[i] = np.array(prodId[i])\n",
        "\n",
        "print(prodId.shape)\n",
        "\n",
        "cateId = np.array(cateId)\n",
        "\n",
        "for i in range(len(cateId)):\n",
        "  cateId[i] = np.array(cateId[i])\n",
        "\n",
        "print(cateId.shape)\n",
        "\n",
        "print(prodId[0].shape)\n",
        "print(cateId[0].shape)\n",
        "\n",
        "prodId = prodId.reshape(1,prodId.shape[0])\n",
        "cateId = cateId.reshape(1,cateId.shape[0])\n",
        "print(prodId.shape)\n",
        "print(cateId.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(991,)\n",
            "(991,)\n",
            "(4,)\n",
            "(4,)\n",
            "(1, 991)\n",
            "(1, 991)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr66CcuHXDvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "candProd = candProd.reshape(1,candProd.shape[0])\n",
        "candCate = candCate.reshape(1,candCate.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Og9gx0YGCHZ",
        "colab_type": "code",
        "outputId": "84de993d-f83f-46b0-f5d3-71db9a540cd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        }
      },
      "source": [
        "from keras.models import Model\n",
        "import numpy as np\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import  AveragePooling1D, Flatten, Dense, Dropout, Embedding, Lambda, Input, Concatenate, PReLU\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.metrics import binary_accuracy \n",
        "from keras import regularizers\n",
        "import tensorflow as tf\n",
        "\n",
        "embedOutDim = 200\n",
        "poolSize = 2\n",
        "\n",
        "##################################################################\n",
        "prodInput = Input(shape=(None,), name= 'products') #batch_shape(None, None)\n",
        "prodEmbed =  Embedding(input_dim =numberOfProducts ,output_dim= embedOutDim, embeddings_regularizer= regularizers.l2(0.01)) (prodInput)\n",
        "\n",
        "##################################################################\n",
        "cateInput = Input(shape=(None,), name= 'categories')\n",
        "cateEmbed =  Embedding(input_dim = numberOfCategories , output_dim=embedOutDim, embeddings_regularizer= regularizers.l2(0.01)) (cateInput)\n",
        "\n",
        "##################################################################\n",
        "candProdInput = Input(shape=(None,),  name= 'candidateProdId')\n",
        "candProdEmbed =  Embedding(input_dim = numberOfProducts , output_dim=embedOutDim, embeddings_regularizer= regularizers.l2(0.01)) (candProdInput)\n",
        "\n",
        "\n",
        "##################################################################\n",
        "candCateInput = Input(shape=(None,),  name= 'candidateCateId')\n",
        "candCateEmbed =  Embedding(input_dim = numberOfCategories , output_dim=embedOutDim, embeddings_regularizer= regularizers.l2(0.01)) (candCateInput)\n",
        "\n",
        "Goods       = Concatenate()([prodEmbed,cateEmbed])\n",
        "pooledGoods = AveragePooling1D(pool_size=poolSize, padding='same')(Goods)\n",
        "\n",
        "Candidate = Concatenate()([candProdEmbed,candCateEmbed])\n",
        "#print(tf.shape(Candidate))\n",
        "\n",
        "Concatendated = Concatenate(axis=-2)([pooledGoods,Candidate])\n",
        "#tf.reshape(Concatendated, (Concatendated.shape[0]*Concatendated.shape[1]*Concatendated.shape[2]))\n",
        "\n",
        "Flattened     = Lambda(lambda x: tf.keras.backend.batch_flatten(x))(Concatendated)\n",
        "\n",
        "Dense1  = Dense(200)(Flattened)        #(Concatendated)\n",
        "Dense1  = PReLU()(Dense1)\n",
        "\n",
        "Dense2  = Dense(80)(Dense1)\n",
        "Dense2  = PReLU()(Dense2)\n",
        "\n",
        "CTR     = Dense(1, activation='softmax',use_bias = False, name= 'ctr')(Dense2)\n",
        "\n",
        "model = Model(inputs=[userInput, prodInput, cateInput,candProdInput, candCateInput ], outputs= [CTR])\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit([userId, prodId, cateId, candProd, candCate], 1, epochs = 5)\n",
        "#print(userEmbed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-62860ba8f77a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mFlattened\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConcatendated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mDense1\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlattened\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m#(Concatendated)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mDense1\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mPReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    893\u001b[0m                                       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                                       \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                                       constraint=self.kernel_constraint)\n\u001b[0m\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             self.bias = self.add_weight(shape=(self.units,),\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         weight = K.variable(initializer(shape, dtype=dtype),\n\u001b[0m\u001b[1;32m    244\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mscale\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfan_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0mscale\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfan_in\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfan_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'normal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# 0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv1x4LdAx1Ho",
        "colab_type": "text"
      },
      "source": [
        "#New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QoHaSio9dMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# convert from json to pandas\n",
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    yield json.loads(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "# make dict and fit it by ids\n",
        "def dict_ids(col):\n",
        "  id = 0\n",
        "  dic = dict() \n",
        "  for i in col:\n",
        "    if i in dic:\n",
        "      dic[i] = dic.get(i)\n",
        "    else:\n",
        "      dic[i] = id\n",
        "      id += 1\n",
        "  return dic\n",
        "\n",
        "# make new col by mapping last ids to new ids\n",
        "def new_col(col,dic):\n",
        "  lis = list()\n",
        "  for i in col:\n",
        "    #if i in dic:\n",
        "    lis.append(dic.get(i))\n",
        "  return lis\n",
        "\n",
        "df_all_beauty = getDF('/content/drive/My Drive/All Beauty/All_Beauty_5.json.gz')\n",
        "#df_amazon_fashion = getDF('/content/drive/My Drive/Amazon Fashion/AMAZON_FASHION_5.json.gz')\n",
        "\n",
        "\"\"\"#df_appliances = getDF('/content/drive/My Drive/Appliances/Appliances_5.json.gz')\n",
        "df_books = getDF('/content/drive/My Drive/Books/Books_5.json.gz')\n",
        "df_cell_phones = getDF('/content/drive/My Drive/Cell Phones and Accessories/Cell_Phones_and_Accessories_5.json.gz')\n",
        "df_digital_music = getDF('/content/drive/My Drive/Digital Music/Copy of Digital_Music_5.json.gz') \n",
        "df_electronics = getDF('/content/drive/My Drive/Electronics/Copy of Electronics_5.json.gz')\n",
        "df_musical_instruments = getDF('/content/drive/My Drive/Musical Instruments/Copy of Musical_Instruments_5.json.gz')\n",
        "df_software = getDF('/content/drive/My Drive/Software/Copy of Software_5.json.gz')\n",
        "df_video_games = getDF('/content/drive/My Drive/Video Games/Video_Games_5.json.gz')\n",
        "\"\"\"\n",
        "#userId = np.array(df_all_beauty['reviewerID'])\n",
        "\n",
        "dict_all_beauty = dict_ids(userId)\n",
        "# dict_amazon_fashion = dict_ids(df_amazon_fashion['reviewerID'])\n",
        "\"\"\"#dict_appliances = dict_ids(df_appliances['reviewerID'])\n",
        "dict_books = dict_ids(df_books['reviewerID'])\n",
        "dict_cell_phones = dict_ids(cell_phones['reviewerID'])\n",
        "dict_digital_music = dict_ids(df_digital_music['reviewerID'])\n",
        "dict_electronics = dict_ids(df_electronics['reviewerID'])\n",
        "dict_musical_instruments = dict_ids(df_musical_instruments['reviewerID'])\n",
        "dict_software = dict_ids(df_software['reviewerID'])\n",
        "dict_video_games = dict_ids(df_video_games['reviewerID'])\n",
        "\"\"\"\n",
        "#productId = np.array(df_all_beauty['asin'])\n",
        "\n",
        "dict_user_all_beauty = dict_ids(df_all_beauty['asin'])\n",
        "dict_user_amazon_fashion = dict_ids(df_amazon_fashion['asin'])\n",
        "\"\"\"#dict_user_appliances = dict_ids(df_appliances['asin'])\n",
        "dict_user_books = dict_ids(df_books['asin'])\n",
        "dict_user_cell_phones = dict_ids(cell_phones['asin'])\n",
        "dict_user_digital_music = dict_ids(df_digital_music['asin'])\n",
        "dict_user_electronics = dict_ids(df_electronics['asin'])\n",
        "dict_user_musical_instruments = dict_ids(df_musical_instruments['asin'])\n",
        "dict_user_software = dict_ids(df_software['asin'])\n",
        "dict_user_video_games = dict_ids(df_video_games['asin'])\n",
        "\"\"\"\n",
        "df_all_beauty['userID']=new_col(df_all_beauty['reviewerID'],dict_all_beauty)\n",
        "#df_amazon_fashion['userID']=new_col(df_amazon_fashion['reviewerID'],dict_amazon_fashion)\n",
        "\"\"\"#df_appliances['userID']=new_col(df_appliances['reviewerID'],dict_appliances)\n",
        "df_books['userID']=new_col(df_books['reviewerID'],dict_books)\n",
        "df_cell_phones['userID']=new_col(df_cell_phones['reviewerID'],dict_cell_phones)\n",
        "df_digital_music['userID']=new_col(df_digital_music['reviewerID'],dict_digital_music)\n",
        "df_electronics['userID']=new_col(df_electronics['reviewerID'],dict_electronics)\n",
        "df_musical_instruments['userID']=new_col(df_musical_instruments['reviewerID'],dict_musical_instruments)\n",
        "df_software['userID']=new_col(df_software['reviewerID'],dict_software)\n",
        "df_video_games['userID']=new_col(df_video_games['reviewerID'],dict_video_games)\n",
        "\"\"\"\n",
        "df_all_beauty['productID']=new_col(df_all_beauty['asin'],dict_user_all_beauty)\n",
        "#df_amazon_fashion['productID']=new_col(df_amazon_fashion['asin'],dict_user_amazon_fashion)\n",
        "\"\"\"#df_appliances['productID']=new_col(df_appliances['asin'],dict_user_appliances)\n",
        "df_books['productID']=new_col(df_books['asin'],dict_user_books)\n",
        "df_cell_phones['productID']=new_col(df_cell_phones['asin'],dict_user_cell_phones)\n",
        "df_digital_music['productID']=new_col(df_digital_music['asin'],dict_user_digital_music)\n",
        "df_electronics['productID']=new_col(df_electronics['asin'],dict_user_electronics)\n",
        "df_musical_instruments['productID']=new_col(df_musical_instruments['asin'],dict_user_musical_instruments)\n",
        "df_software['productID']=new_col(df_software['asin'],dict_user_software)\n",
        "df_video_games['productID']=new_col(df_video_games['asin'],dict_user_video_games)\n",
        "\"\"\"\n",
        "# this function  not finished \n",
        "def listofproduct(productID,userID):\n",
        "  dic = dict()\n",
        "  for i in range(0,len(userID)):\n",
        "    dic[userID[i]]\n",
        "  return dic  \n",
        "\n",
        "\n",
        "print(df_all_beauty['reviewerID'])\n",
        "print(df_all_beauty['userID'])\n",
        "#print(df_amazon_fashion)\n",
        "# print(listofproduct(df_all_beauty['productID'],df_all_beauty['userID']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeLEQvvPUWwv",
        "colab_type": "text"
      },
      "source": [
        "#Implementations\n",
        "\n",
        "Implementation 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8db1peXjS7tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding:utf-8 -*-\n",
        "\"\"\"\n",
        "Author:\n",
        "    Weichen Shen,wcshen1994@163.com\n",
        "\n",
        "Reference:\n",
        "    [1] Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068. (https://arxiv.org/pdf/1706.06978.pdf)\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.python.keras.layers import Dense,Concatenate, Flatten\n",
        "from tensorflow.python.keras.models import Model\n",
        "\n",
        "from ..inputs import  build_input_features,create_embedding_matrix,SparseFeat,VarLenSparseFeat,DenseFeat,embedding_lookup,get_dense_input,varlen_embedding_lookup,get_varlen_pooling_list,combined_dnn_input\n",
        "from ..layers.core import DNN, PredictionLayer\n",
        "from ..layers.sequence import AttentionSequencePoolingLayer\n",
        "from ..layers.utils import concat_func, NoMask\n",
        "\n",
        "\n",
        "def DIN(dnn_feature_columns, history_feature_list, dnn_use_bn=False,\n",
        "        dnn_hidden_units=(200, 80), dnn_activation='relu', att_hidden_size=(80, 40), att_activation=\"dice\",\n",
        "        att_weight_normalization=False, l2_reg_dnn=0, l2_reg_embedding=1e-6, dnn_dropout=0, init_std=0.0001, seed=1024,\n",
        "        task='binary'):\n",
        "    \"\"\"Instantiates the Deep Interest Network architecture.\n",
        "\n",
        "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
        "    :param history_feature_list: list,to indicate  sequence sparse field\n",
        "    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in deep net\n",
        "    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net\n",
        "    :param dnn_activation: Activation function to use in deep net\n",
        "    :param att_hidden_size: list,list of positive integer , the layer number and units in each layer of attention net\n",
        "    :param att_activation: Activation function to use in attention net\n",
        "    :param att_weight_normalization: bool.Whether normalize the attention score of local activation unit.\n",
        "    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n",
        "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
        "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
        "    :param init_std: float,to use as the initialize std of embedding vector\n",
        "    :param seed: integer ,to use as random seed.\n",
        "    :param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
        "    :return: A Keras model instance.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    features = build_input_features(dnn_feature_columns)\n",
        "\n",
        "    sparse_feature_columns = list(filter(lambda x:isinstance(x,SparseFeat),dnn_feature_columns)) if dnn_feature_columns else []\n",
        "    dense_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, DenseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
        "    varlen_sparse_feature_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
        "\n",
        "\n",
        "    history_feature_columns = []\n",
        "    sparse_varlen_feature_columns = []\n",
        "    history_fc_names = list(map(lambda x: \"hist_\" + x, history_feature_list))\n",
        "    for fc in varlen_sparse_feature_columns:\n",
        "        feature_name = fc.name\n",
        "        if feature_name in history_fc_names:\n",
        "            history_feature_columns.append(fc)\n",
        "        else:\n",
        "            sparse_varlen_feature_columns.append(fc)\n",
        "\n",
        "\n",
        "    inputs_list = list(features.values())\n",
        "\n",
        "\n",
        "    embedding_dict = create_embedding_matrix(dnn_feature_columns, l2_reg_embedding, init_std, seed, prefix=\"\")\n",
        "\n",
        "\n",
        "    query_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns, history_feature_list,\n",
        "                                      history_feature_list,to_list=True)\n",
        "    keys_emb_list = embedding_lookup(embedding_dict, features, history_feature_columns, history_fc_names,\n",
        "                                     history_fc_names,to_list=True)\n",
        "    dnn_input_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns,\n",
        "                                          mask_feat_list=history_feature_list,to_list=True)\n",
        "    dense_value_list = get_dense_input(features, dense_feature_columns)\n",
        "\n",
        "    sequence_embed_dict = varlen_embedding_lookup(embedding_dict,features,sparse_varlen_feature_columns)\n",
        "    sequence_embed_list = get_varlen_pooling_list(sequence_embed_dict, features, sparse_varlen_feature_columns,to_list=True)\n",
        "\n",
        "    dnn_input_emb_list += sequence_embed_list\n",
        "\n",
        "\n",
        "    keys_emb = concat_func(keys_emb_list, mask=True)\n",
        "    deep_input_emb = concat_func(dnn_input_emb_list)\n",
        "    query_emb = concat_func(query_emb_list, mask=True)\n",
        "    hist = AttentionSequencePoolingLayer(att_hidden_size, att_activation,\n",
        "                                         weight_normalization=att_weight_normalization, supports_masking=True)([\n",
        "        query_emb, keys_emb])\n",
        "\n",
        "    deep_input_emb = Concatenate()([NoMask()(deep_input_emb), hist])\n",
        "    deep_input_emb = Flatten()(deep_input_emb)\n",
        "    dnn_input = combined_dnn_input([deep_input_emb],dense_value_list)\n",
        "    output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn,\n",
        "                 dnn_dropout, dnn_use_bn, seed)(dnn_input)\n",
        "    final_logit = Dense(1, use_bias=False)(output)\n",
        "\n",
        "    output = PredictionLayer(task)(final_logit)\n",
        "\n",
        "    model = Model(inputs=inputs_list, outputs=output)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeQ30jyGUNQk",
        "colab_type": "text"
      },
      "source": [
        "Implementation 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLj-XxTGUKlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from Dice import dice\n",
        "\n",
        "class Model(object):\n",
        "\n",
        "  def __init__(self, user_count, item_count, cate_count, cate_list, predict_batch_size, predict_ads_num):\n",
        "\n",
        "    self.u = tf.placeholder(tf.int32, [None,]) # [B]\n",
        "    self.i = tf.placeholder(tf.int32, [None,]) # [B]\n",
        "    self.j = tf.placeholder(tf.int32, [None,]) # [B]\n",
        "    self.y = tf.placeholder(tf.float32, [None,]) # [B]\n",
        "    self.hist_i = tf.placeholder(tf.int32, [None, None]) # [B, T]\n",
        "    self.sl = tf.placeholder(tf.int32, [None,]) # [B]\n",
        "    self.lr = tf.placeholder(tf.float64, [])\n",
        "\n",
        "    hidden_units = 128\n",
        "\n",
        "    user_emb_w = tf.get_variable(\"user_emb_w\", [user_count, hidden_units])\n",
        "    item_emb_w = tf.get_variable(\"item_emb_w\", [item_count, hidden_units // 2])\n",
        "    item_b = tf.get_variable(\"item_b\", [item_count],\n",
        "                             initializer=tf.constant_initializer(0.0))\n",
        "    cate_emb_w = tf.get_variable(\"cate_emb_w\", [cate_count, hidden_units // 2])\n",
        "    cate_list = tf.convert_to_tensor(cate_list, dtype=tf.int64)\n",
        "\n",
        "    ic = tf.gather(cate_list, self.i)\n",
        "    i_emb = tf.concat(values = [\n",
        "        tf.nn.embedding_lookup(item_emb_w, self.i),\n",
        "        tf.nn.embedding_lookup(cate_emb_w, ic),\n",
        "        ], axis=1)\n",
        "    i_b = tf.gather(item_b, self.i)\n",
        "\n",
        "    jc = tf.gather(cate_list, self.j)\n",
        "    j_emb = tf.concat([\n",
        "        tf.nn.embedding_lookup(item_emb_w, self.j),\n",
        "        tf.nn.embedding_lookup(cate_emb_w, jc),\n",
        "        ], axis=1)\n",
        "    j_b = tf.gather(item_b, self.j)\n",
        "\n",
        "    hc = tf.gather(cate_list, self.hist_i)\n",
        "    h_emb = tf.concat([\n",
        "        tf.nn.embedding_lookup(item_emb_w, self.hist_i),\n",
        "        tf.nn.embedding_lookup(cate_emb_w, hc),\n",
        "        ], axis=2)\n",
        "\n",
        "    hist_i =attention(i_emb, h_emb, self.sl)\n",
        "    #-- attention end ---\n",
        "    \n",
        "    hist_i = tf.layers.batch_normalization(inputs = hist_i)\n",
        "    hist_i = tf.reshape(hist_i, [-1, hidden_units], name='hist_bn')\n",
        "    hist_i = tf.layers.dense(hist_i, hidden_units, name='hist_fcn')\n",
        "\n",
        "    u_emb_i = hist_i\n",
        "    \n",
        "    hist_j =attention(j_emb, h_emb, self.sl)\n",
        "    #-- attention end ---\n",
        "    \n",
        "    hist_j = tf.layers.batch_normalization(inputs = hist_j)\n",
        "    hist_j = tf.reshape(hist_j, [-1, hidden_units], name='hist_bn')\n",
        "    hist_j = tf.layers.dense(hist_j, hidden_units, name='hist_fcn', reuse=True)\n",
        "\n",
        "    u_emb_j = hist_j\n",
        "    print u_emb_i.get_shape().as_list()\n",
        "    print u_emb_j.get_shape().as_list()\n",
        "    print i_emb.get_shape().as_list()\n",
        "    print j_emb.get_shape().as_list()\n",
        "    #-- fcn begin -------\n",
        "    din_i = tf.concat([u_emb_i, i_emb], axis=-1)\n",
        "    din_i = tf.layers.batch_normalization(inputs=din_i, name='b1')\n",
        "    d_layer_1_i = tf.layers.dense(din_i, 80, activation=tf.nn.sigmoid, name='f1')\n",
        "    #if u want try dice change sigmoid to None and add dice layer like following two lines. You can also find model_dice.py in this folder.\n",
        "    # d_layer_1_i = tf.layers.dense(din_i, 80, activation=None, name='f1')\n",
        "    # d_layer_1_i = dice(d_layer_1_i, name='dice_1')\n",
        "    # d_layer_2_i = tf.layers.dense(d_layer_1_i, 40, activation=None, name='f2')\n",
        "    d_layer_2_i = tf.layers.dense(d_layer_1_i, 40, activation=tf.nn.sigmoid, name='f2')\n",
        "    #d_layer_2_i = dice(d_layer_2_i, name='dice_2')\n",
        "    d_layer_3_i = tf.layers.dense(d_layer_2_i, 1, activation=None, name='f3')\n",
        "    din_j = tf.concat([u_emb_j, j_emb], axis=-1)\n",
        "    din_j = tf.layers.batch_normalization(inputs=din_j, name='b1', reuse=True)\n",
        "    d_layer_1_j = tf.layers.dense(din_j, 80, activation=tf.nn.sigmoid, name='f1', reuse=True)\n",
        "    # d_layer_1_j = tf.layers.dense(din_j, 80, activation=None, name='f1', reuse=True)\n",
        "    # d_layer_1_j = dice(d_layer_1_j, name='dice_1')\n",
        "    d_layer_2_j = tf.layers.dense(d_layer_1_j, 40, activation=tf.nn.sigmoid, name='f2', reuse=True)\n",
        "    # d_layer_2_j = tf.layers.dense(d_layer_1_j, 40, activation=None, name='f2', reuse=True)\n",
        "    # d_layer_2_j = dice(d_layer_2_j, name='dice_2')\n",
        "    d_layer_3_j = tf.layers.dense(d_layer_2_j, 1, activation=None, name='f3', reuse=True)\n",
        "    d_layer_3_i = tf.reshape(d_layer_3_i, [-1])\n",
        "    d_layer_3_j = tf.reshape(d_layer_3_j, [-1])\n",
        "    x = i_b - j_b + d_layer_3_i - d_layer_3_j # [B]\n",
        "    self.logits = i_b + d_layer_3_i\n",
        "    \n",
        "    # prediciton for selected items\n",
        "    # logits for selected item:\n",
        "    item_emb_all = tf.concat([\n",
        "        item_emb_w,\n",
        "        tf.nn.embedding_lookup(cate_emb_w, cate_list)\n",
        "        ], axis=1)\n",
        "    item_emb_sub = item_emb_all[:predict_ads_num,:]\n",
        "    item_emb_sub = tf.expand_dims(item_emb_sub, 0)\n",
        "    item_emb_sub = tf.tile(item_emb_sub, [predict_batch_size, 1, 1])\n",
        "    hist_sub =attention_multi_items(item_emb_sub, h_emb, self.sl)\n",
        "    #-- attention end ---\n",
        "    \n",
        "    hist_sub = tf.layers.batch_normalization(inputs = hist_sub, name='hist_bn', reuse=tf.AUTO_REUSE)\n",
        "    # print hist_sub.get_shape().as_list() \n",
        "    hist_sub = tf.reshape(hist_sub, [-1, hidden_units])\n",
        "    hist_sub = tf.layers.dense(hist_sub, hidden_units, name='hist_fcn', reuse=tf.AUTO_REUSE)\n",
        "\n",
        "    u_emb_sub = hist_sub\n",
        "    item_emb_sub = tf.reshape(item_emb_sub, [-1, hidden_units])\n",
        "    din_sub = tf.concat([u_emb_sub, item_emb_sub], axis=-1)\n",
        "    din_sub = tf.layers.batch_normalization(inputs=din_sub, name='b1', reuse=True)\n",
        "    d_layer_1_sub = tf.layers.dense(din_sub, 80, activation=tf.nn.sigmoid, name='f1', reuse=True)\n",
        "    #d_layer_1_sub = dice(d_layer_1_sub, name='dice_1_sub')\n",
        "    d_layer_2_sub = tf.layers.dense(d_layer_1_sub, 40, activation=tf.nn.sigmoid, name='f2', reuse=True)\n",
        "    #d_layer_2_sub = dice(d_layer_2_sub, name='dice_2_sub')\n",
        "    d_layer_3_sub = tf.layers.dense(d_layer_2_sub, 1, activation=None, name='f3', reuse=True)\n",
        "    d_layer_3_sub = tf.reshape(d_layer_3_sub, [-1, predict_ads_num])\n",
        "    self.logits_sub = tf.sigmoid(item_b[:predict_ads_num] + d_layer_3_sub)\n",
        "    self.logits_sub = tf.reshape(self.logits_sub, [-1, predict_ads_num, 1])\n",
        "    #-- fcn end -------\n",
        "\n",
        "    \n",
        "    self.mf_auc = tf.reduce_mean(tf.to_float(x > 0))\n",
        "    self.score_i = tf.sigmoid(i_b + d_layer_3_i)\n",
        "    self.score_j = tf.sigmoid(j_b + d_layer_3_j)\n",
        "    self.score_i = tf.reshape(self.score_i, [-1, 1])\n",
        "    self.score_j = tf.reshape(self.score_j, [-1, 1])\n",
        "    self.p_and_n = tf.concat([self.score_i, self.score_j], axis=-1)\n",
        "    print self.p_and_n.get_shape().as_list()\n",
        "\n",
        "\n",
        "    # Step variable\n",
        "    self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "    self.global_epoch_step = \\\n",
        "        tf.Variable(0, trainable=False, name='global_epoch_step')\n",
        "    self.global_epoch_step_op = \\\n",
        "        tf.assign(self.global_epoch_step, self.global_epoch_step+1)\n",
        "\n",
        "    self.loss = tf.reduce_mean(\n",
        "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=self.logits,\n",
        "            labels=self.y)\n",
        "        )\n",
        "\n",
        "    trainable_params = tf.trainable_variables()\n",
        "    self.opt = tf.train.GradientDescentOptimizer(learning_rate=self.lr)\n",
        "    gradients = tf.gradients(self.loss, trainable_params)\n",
        "    clip_gradients, _ = tf.clip_by_global_norm(gradients, 5)\n",
        "    self.train_op = self.opt.apply_gradients(\n",
        "        zip(clip_gradients, trainable_params), global_step=self.global_step)\n",
        "\n",
        "\n",
        "  def train(self, sess, uij, l):\n",
        "    loss, _ = sess.run([self.loss, self.train_op], feed_dict={\n",
        "        self.u: uij[0],\n",
        "        self.i: uij[1],\n",
        "        self.y: uij[2],\n",
        "        self.hist_i: uij[3],\n",
        "        self.sl: uij[4],\n",
        "        self.lr: l,\n",
        "        })\n",
        "    return loss\n",
        "\n",
        "  def eval(self, sess, uij):\n",
        "    u_auc, socre_p_and_n = sess.run([self.mf_auc, self.p_and_n], feed_dict={\n",
        "        self.u: uij[0],\n",
        "        self.i: uij[1],\n",
        "        self.j: uij[2],\n",
        "        self.hist_i: uij[3],\n",
        "        self.sl: uij[4],\n",
        "        })\n",
        "    return u_auc, socre_p_and_n\n",
        "  \n",
        "  def test(self, sess, uij):\n",
        "    return sess.run(self.logits_sub, feed_dict={\n",
        "        self.u: uij[0],\n",
        "        self.i: uij[1],\n",
        "        self.j: uij[2],\n",
        "        self.hist_i: uij[3],\n",
        "        self.sl: uij[4],\n",
        "        })\n",
        "  \n",
        "\n",
        "  def save(self, sess, path):\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, save_path=path)\n",
        "\n",
        "  def restore(self, sess, path):\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, save_path=path)\n",
        "\n",
        "def extract_axis_1(data, ind):\n",
        "  batch_range = tf.range(tf.shape(data)[0])\n",
        "  indices = tf.stack([batch_range, ind], axis=1)\n",
        "  res = tf.gather_nd(data, indices)\n",
        "  return res\n",
        "\n",
        "def attention(queries, keys, keys_length):\n",
        "  '''\n",
        "    queries:     [B, H]\n",
        "    keys:        [B, T, H]\n",
        "    keys_length: [B]\n",
        "  '''\n",
        "  queries_hidden_units = queries.get_shape().as_list()[-1]\n",
        "  queries = tf.tile(queries, [1, tf.shape(keys)[1]])\n",
        "  queries = tf.reshape(queries, [-1, tf.shape(keys)[1], queries_hidden_units])\n",
        "  din_all = tf.concat([queries, keys, queries-keys, queries*keys], axis=-1)\n",
        "  d_layer_1_all = tf.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name='f1_att', reuse=tf.AUTO_REUSE)\n",
        "  d_layer_2_all = tf.layers.dense(d_layer_1_all, 40, activation=tf.nn.sigmoid, name='f2_att', reuse=tf.AUTO_REUSE)\n",
        "  d_layer_3_all = tf.layers.dense(d_layer_2_all, 1, activation=None, name='f3_att', reuse=tf.AUTO_REUSE)\n",
        "  d_layer_3_all = tf.reshape(d_layer_3_all, [-1, 1, tf.shape(keys)[1]])\n",
        "  outputs = d_layer_3_all \n",
        "  # Mask\n",
        "  key_masks = tf.sequence_mask(keys_length, tf.shape(keys)[1])   # [B, T]\n",
        "  key_masks = tf.expand_dims(key_masks, 1) # [B, 1, T]\n",
        "  paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n",
        "  outputs = tf.where(key_masks, outputs, paddings)  # [B, 1, T]\n",
        "\n",
        "  # Scale\n",
        "  outputs = outputs / (keys.get_shape().as_list()[-1] ** 0.5)\n",
        "\n",
        "  # Activation\n",
        "  outputs = tf.nn.softmax(outputs)  # [B, 1, T]\n",
        "\n",
        "  # Weighted sum\n",
        "  outputs = tf.matmul(outputs, keys)  # [B, 1, H]\n",
        "\n",
        "  return outputs\n",
        "\n",
        "def attention_multi_items(queries, keys, keys_length):\n",
        "  '''\n",
        "    queries:     [B, N, H] N is the number of ads\n",
        "    keys:        [B, T, H] \n",
        "    keys_length: [B]\n",
        "  '''\n",
        "  queries_hidden_units = queries.get_shape().as_list()[-1]\n",
        "  queries_nums = queries.get_shape().as_list()[1]\n",
        "  queries = tf.tile(queries, [1, 1, tf.shape(keys)[1]])\n",
        "  queries = tf.reshape(queries, [-1, queries_nums, tf.shape(keys)[1], queries_hidden_units]) # shape : [B, N, T, H]\n",
        "  max_len = tf.shape(keys)[1]\n",
        "  keys = tf.tile(keys, [1, queries_nums, 1])\n",
        "  keys = tf.reshape(keys, [-1, queries_nums, max_len, queries_hidden_units]) # shape : [B, N, T, H]\n",
        "  din_all = tf.concat([queries, keys, queries-keys, queries*keys], axis=-1)\n",
        "  d_layer_1_all = tf.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name='f1_att', reuse=tf.AUTO_REUSE)\n",
        "  d_layer_2_all = tf.layers.dense(d_layer_1_all, 40, activation=tf.nn.sigmoid, name='f2_att', reuse=tf.AUTO_REUSE)\n",
        "  d_layer_3_all = tf.layers.dense(d_layer_2_all, 1, activation=None, name='f3_att', reuse=tf.AUTO_REUSE)\n",
        "  d_layer_3_all = tf.reshape(d_layer_3_all, [-1, queries_nums, 1, max_len])\n",
        "  outputs = d_layer_3_all \n",
        "  # Mask\n",
        "  key_masks = tf.sequence_mask(keys_length, max_len)   # [B, T]\n",
        "  key_masks = tf.tile(key_masks, [1, queries_nums])\n",
        "  key_masks = tf.reshape(key_masks, [-1, queries_nums, 1, max_len]) # shape : [B, N, 1, T]\n",
        "  paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n",
        "  outputs = tf.where(key_masks, outputs, paddings)  # [B, N, 1, T]\n",
        "\n",
        "  # Scale\n",
        "  outputs = outputs / (keys.get_shape().as_list()[-1] ** 0.5)\n",
        "\n",
        "  # Activation\n",
        "  outputs = tf.nn.softmax(outputs)  # [B, N, 1, T]\n",
        "  outputs = tf.reshape(outputs, [-1, 1, max_len])\n",
        "  keys = tf.reshape(keys, [-1, max_len, queries_hidden_units])\n",
        "  #print outputs.get_shape().as_list()\n",
        "  #print keys.get_sahpe().as_list()\n",
        "  # Weighted sum\n",
        "  outputs = tf.matmul(outputs, keys)\n",
        "  outputs = tf.reshape(outputs, [-1, queries_nums, queries_hidden_units])  # [B, N, 1, H]\n",
        "  print outputs.get_shape().as_list()\n",
        "  return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}